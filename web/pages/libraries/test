# Test Processing

from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

def model_loading(modelo):
    return pipeline("text-generation", model=modelo, trust_remote_code=True, return_full_text=True)

def data_processing(text, pipe, RAG=False, Adv_prompts=False, max_len=100):
    prompt = f"Question: {text}.\nAnswer:"
    generated_text = pipe(prompt, num_return_sequences=1, truncation=True)
    return generated_text[0]["generated_text"].split("\n")[1].strip()


#####################################
import torch
from transformers import pipeline

generate_text = pipeline(model="databricks/dolly-v2-12b", torch_dtype=torch.bfloat16,
                         trust_remote_code=True, device_map="auto", return_full_text=True)

from langchain import PromptTemplate, LLMChain
from langchain.llms import HuggingFacePipeline

# template for an instrution with no input
prompt = PromptTemplate(
    input_variables=["instruction"],
    template="{instruction}")

# template for an instruction with input
prompt_with_context = PromptTemplate(
    input_variables=["instruction", "context"],
    template="{instruction}\n\nInput:\n{context}")

hf_pipeline = HuggingFacePipeline(pipeline=generate_text)

llm_chain = LLMChain(llm=hf_pipeline, prompt=prompt)
llm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)

context = """George Washington (February 22, 1732[b] - December 14, 1799) was an American military officer, statesman,
and Founding Father who served as the first president of the United States from 1789 to 1797."""

print(llm_context_chain.predict(instruction="When was George Washington president?", context=context).lstrip())
